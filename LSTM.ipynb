{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harshjpatel/LSTM/blob/main/LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "c7647894",
      "metadata": {
        "id": "c7647894"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "b4f078b9",
      "metadata": {
        "id": "b4f078b9"
      },
      "outputs": [],
      "source": [
        "# COVID-19 Data Analysis with LSTM Models\n",
        "\n",
        "# This notebook contains functions to analyze COVID-19 data and predict cases using LSTM models. It also provides functions for data preprocessing.\n",
        "\n",
        "## Dependencies\n",
        "# Make sure to install the required dependencies:\n",
        "\n",
        "# ```python\n",
        "# !pip install pandas matplotlib numpy tensorflow\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, Bidirectional\n",
        "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "b6250974",
      "metadata": {
        "id": "b6250974"
      },
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "\n",
        "def parse_JHU_data():\n",
        "    JHU_file = 'data/time_series_covid19_confirmed_US.csv'\n",
        "    case_df = pd.read_csv(JHU_file)\n",
        "    states = sorted(set(case_df['Province_State']))\n",
        "    state_data = pd.DataFrame()\n",
        "    for state in states:\n",
        "        state_rows = case_df[case_df['Province_State'] == state]\n",
        "        state_rows = state_rows.loc[:, '1/22/20':]\n",
        "\n",
        "        state_data[state] = state_rows.sum()\n",
        "\n",
        "    state_data = state_data.reset_index().rename(columns={'index': 'date'}).set_index('date')\n",
        "    state_data.to_csv('data/state_cumulative.csv')\n",
        "    daily_data = state_data.diff().iloc[1:]\n",
        "    daily_data.to_csv('data/state_daily.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "3db8c0a0",
      "metadata": {
        "id": "3db8c0a0"
      },
      "outputs": [],
      "source": [
        "def simple_LSTM():\n",
        "    # from tensorflow import keras\n",
        "    from tensorflow.keras.models import Sequential\n",
        "    from tensorflow.keras.layers import Dense, LSTM, Bidirectional\n",
        "    from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
        "    from tensorflow.keras.regularizers import l2\n",
        "    from tensorflow.keras.callbacks import EarlyStopping\n",
        "    from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "    state, state_short = 'Texas', 'TX'\n",
        "    daily_file = 'data/state_daily.csv'\n",
        "    daily_df = pd.read_csv(daily_file)\n",
        "    daily_df['date'] = pd.to_datetime(daily_df['date'])\n",
        "    daily_df = daily_df[['date', state]]\n",
        "    moving_avg = daily_df[state].rolling(window=7).mean().iloc[6:]\n",
        "    daily_df[state] = moving_avg\n",
        "\n",
        "    mobility_file = 'data/mobility.csv'\n",
        "    mobility_df = pd.read_csv(mobility_file)\n",
        "    mobility_df['date'] = pd.to_datetime(mobility_df['date'])\n",
        "    mobility_df = mobility_df[mobility_df['state'] == state_short]\n",
        "    mobility_df.drop(columns=['state'], inplace=True)\n",
        "\n",
        "    tweet_file = 'data/parseTwitterData.csv'\n",
        "    tweet_df = pd.read_csv(tweet_file)\n",
        "    tweet_df.rename(columns={'Date': 'date'}, inplace=True)\n",
        "    tweet_df['date'] = pd.to_datetime(tweet_df['date'])\n",
        "    tweet_df = tweet_df[tweet_df['State'] == state_short]\n",
        "    tweet_df = tweet_df[\n",
        "        ['date', 'StringencyIndex_WeightedAverage',\n",
        "         'Tweet_CPL', 'Tweet_AS', 'Reply_swear', 'Reply_emo_anger']]\n",
        "\n",
        "    X = daily_df.copy()\n",
        "    X = pd.merge(X, mobility_df, on='date', how='outer')\n",
        "    X = pd.merge(X, tweet_df, on='date', how='outer')\n",
        "\n",
        "    # add lagged Y columns\n",
        "    prediction_length = 7\n",
        "    for i in range(prediction_length):\n",
        "        X[f'Y{i}'] = X[state].shift(-i)\n",
        "\n",
        "    X.dropna(inplace=True)\n",
        "    display(X[X[state] <= 0])\n",
        "    dates = X['date']\n",
        "    X.drop(columns=['date'], inplace=True)\n",
        "\n",
        "    # select the target columns\n",
        "    Y = X.iloc[:, -prediction_length:].values\n",
        "    # drop the target\n",
        "    X = X.iloc[:, :-prediction_length]\n",
        "    n_features = X.shape[-1]\n",
        "    X = X.values\n",
        "    X_train, X_CV = X[:450], X[450:]\n",
        "    Y_train, Y_CV = Y[:450], Y[450:]\n",
        "    dates_train, dates_CV = dates[:450], dates[450:]\n",
        "\n",
        "    window_size = 14\n",
        "    generator_train = TimeseriesGenerator(X_train, Y_train, length=window_size)\n",
        "    generator_CV = TimeseriesGenerator(X_CV, Y_CV, length=window_size)\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Bidirectional(LSTM(64,\n",
        "                                 activation='relu',\n",
        "                                 input_shape=(window_size, n_features),\n",
        "                                 kernel_regularizer=l2(0.01), recurrent_regularizer=l2(0.01)\n",
        "                                 )))\n",
        "    model.add(Dense(64, activation='relu', kernel_regularizer=l2(0.01)))\n",
        "    model.add(Dense(prediction_length))\n",
        "    model.build(input_shape=(None, window_size, n_features))\n",
        "    model.compile(optimizer='adam', loss='mape',  metrics=['accuracy'])\n",
        "    model.summary()\n",
        "\n",
        "    model.fit(generator_train, epochs=100, batch_size=100)\n",
        "    Y_pred_train = model.predict(generator_train)\n",
        "    Y_pred_CV = model.predict(generator_CV)\n",
        "\n",
        "    display(len(Y_pred_CV))\n",
        "\n",
        "    mean_sqr_error = np.mean(\n",
        "        [np.abs(Y_train[window_size:] - Y_pred_train) / Y_train[window_size:]]\n",
        "    )\n",
        "    display('training MAPE:', f'{100 * mean_sqr_error:.4f}')\n",
        "\n",
        "    mean_sqr_error = np.mean(\n",
        "        [np.abs(Y_CV[window_size:] - Y_pred_CV) / Y_CV[window_size:]]\n",
        "    )\n",
        "    display('validation MAPE:', f'{100 * mean_sqr_error:.4f}')\n",
        "\n",
        "    for i in range(prediction_length):\n",
        "        fig = plt.figure()\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        ax1 = fig.add_subplot(1, 2, 1)\n",
        "        ax1.plot(dates_train[window_size:], Y_train[window_size:, i], label='Y')\n",
        "        ax1.plot(dates_train[window_size:], Y_pred_train[:, i], label='prediction')\n",
        "        ax1.legend()\n",
        "        ax1.set_title('training')\n",
        "        ax2 = fig.add_subplot(1, 2, 2)\n",
        "        ax2.plot(dates_CV[window_size:], Y_CV[window_size:, i], label='Y')\n",
        "        ax2.plot(dates_CV[window_size:], Y_pred_CV[:, i], label='prediction')\n",
        "        ax2.legend()\n",
        "        ax2.set_title('CV')\n",
        "        fig.autofmt_xdate()\n",
        "        plt.show()\n",
        "\n",
        "    for i in range(7):\n",
        "        fig = plt.figure()\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        ax1 = fig.add_subplot()\n",
        "        ax1.plot(dates_CV[window_size + i:window_size + i + prediction_length], Y_CV[window_size + i], label='Y')\n",
        "        ax1.plot(dates_CV[window_size + i:window_size + i + prediction_length], Y_pred_CV[i], label='prediction')\n",
        "        ax1.legend()\n",
        "        fig.autofmt_xdate()\n",
        "        plt.show()\n",
        "        plt.close(fig)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "a7a4e781",
      "metadata": {
        "id": "a7a4e781"
      },
      "outputs": [],
      "source": [
        "def parse_mobility():\n",
        "    mobility_files = ['data/Mobility/2020_US_Region_Mobility_Report.csv',\n",
        "                      'data/Mobility/2021_US_Region_Mobility_Report.csv',\n",
        "                      'data/Mobility/2022_US_Region_Mobility_Report.csv']\n",
        "    mobility_df = pd.DataFrame()\n",
        "    for file in mobility_files:\n",
        "        df = pd.read_csv(file)\n",
        "        cols_to_read = [5] + list(range(8, len(df.columns)))\n",
        "        df = df.iloc[:, cols_to_read]\n",
        "        df.iloc[:, 0] = df.iloc[:, 0].astype(str)\n",
        "        # df.iloc[:, 1] = pd.to_datetime((df.iloc[:, 1]))\n",
        "        mobility_df = pd.concat([mobility_df, df], ignore_index=True)\n",
        "\n",
        "    print(mobility_df.info())\n",
        "    mobility_df.rename(columns={mobility_df.columns[0]: 'state'}, inplace=True)\n",
        "    mobility_df = mobility_df[mobility_df['state'] != 'nan']\n",
        "    mobility_df['state'] = mobility_df['state'].apply(lambda x: x[3:])\n",
        "    mobility_df.to_csv('data/mobility.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "fd08a975",
      "metadata": {
        "id": "fd08a975"
      },
      "outputs": [],
      "source": [
        "def tmp():\n",
        "    daily_file = 'data/state_daily.csv'\n",
        "    daily_df = pd.read_csv(daily_file)\n",
        "    daily_df['date'] = pd.to_datetime(daily_df['date'])\n",
        "    daily_df = daily_df[['date', 'Illinois']]\n",
        "    moving_avg = daily_df['Illinois'].rolling(window=7).mean().iloc[6:]\n",
        "    daily_df['Illinois'] = moving_avg\n",
        "\n",
        "    mobility_file = 'data/mobility.csv'\n",
        "    mobility_df = pd.read_csv(mobility_file)\n",
        "    mobility_df['date'] = pd.to_datetime(mobility_df['date'])\n",
        "    mobility_df = mobility_df[mobility_df['state'] == 'IL']\n",
        "    mobility_df.drop(columns=['state'], inplace=True)\n",
        "\n",
        "    tweet_file = 'data/parseTwitterData.csv'\n",
        "    tweet_df = pd.read_csv(tweet_file)\n",
        "    tweet_df.rename(columns={'Date': 'date'}, inplace=True)\n",
        "    tweet_df['date'] = pd.to_datetime(tweet_df['date'])\n",
        "    tweet_df = tweet_df[tweet_df['State'] == 'IL']\n",
        "    tweet_df = tweet_df[\n",
        "        ['date', 'StringencyIndex_WeightedAverage', 'Tweet_CPL', 'Tweet_AS', 'Reply_swear', 'Reply_emo_anger']]\n",
        "\n",
        "    X = pd.merge(daily_df, mobility_df, on='date', how='outer')\n",
        "    X = pd.merge(X, tweet_df, on='date', how='outer')\n",
        "    # scaler = StandardScaler()\n",
        "    # scaler.fit(daily_df)\n",
        "    X_states = ['Illinois', 'Indiana', 'Ohio', 'Wisconsin']\n",
        "    # X_states = ['Illinois']\n",
        "    # Y_states = ['Wisconsin']\n",
        "    Y_states = ['Illinois']\n",
        "    n_features = X.shape[-1]\n",
        "    X.dropna(inplace=True)\n",
        "    print(X.head())\n",
        "    Y = X['Illinois'].copy()\n",
        "    print(X.shape, Y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "4de0d928",
      "metadata": {
        "id": "4de0d928"
      },
      "outputs": [],
      "source": [
        "def parseTwitterData():\n",
        "    twitterData = pd.read_csv('data/parseTwitterData.csv')\n",
        "    print(twitterData.info())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "3b1abd93",
      "metadata": {
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "3b1abd93",
        "outputId": "8c364a65-11df-4e7d-dcb1-01a29f6fa7b6"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'data/state_daily.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-1313562f2e0a>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_short\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Texas'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'TX'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mdaily_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'data/state_daily.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mdaily_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdaily_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mdaily_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdaily_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mdaily_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdaily_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    910\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1661\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1662\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1663\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    860\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/state_daily.csv'"
          ]
        }
      ],
      "source": [
        "# simple_LSTM()\n",
        "# from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, Bidirectional\n",
        "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "state, state_short = 'Texas', 'TX'\n",
        "daily_file = 'data/state_daily.csv'\n",
        "daily_df = pd.read_csv(daily_file)\n",
        "daily_df['date'] = pd.to_datetime(daily_df['date'])\n",
        "daily_df = daily_df[['date', state]]\n",
        "moving_avg = daily_df[state].rolling(window=7).mean().iloc[6:]\n",
        "daily_df[state] = moving_avg\n",
        "\n",
        "mobility_file = 'data/mobility.csv'\n",
        "mobility_df = pd.read_csv(mobility_file)\n",
        "mobility_df['date'] = pd.to_datetime(mobility_df['date'])\n",
        "mobility_df = mobility_df[mobility_df['state'] == state_short]\n",
        "mobility_df.drop(columns=['state'], inplace=True)\n",
        "\n",
        "tweet_file = 'data/parseTwitterData.csv'\n",
        "tweet_df = pd.read_csv(tweet_file)\n",
        "tweet_df.rename(columns={'Date': 'date'}, inplace=True)\n",
        "tweet_df['date'] = pd.to_datetime(tweet_df['date'])\n",
        "tweet_df = tweet_df[tweet_df['State'] == state_short]\n",
        "tweet_df = tweet_df[\n",
        "    ['date', 'StringencyIndex_WeightedAverage',\n",
        "      'Tweet_CPL', 'Tweet_AS', 'Reply_swear', 'Reply_emo_anger']]\n",
        "\n",
        "X = daily_df.copy()\n",
        "X = pd.merge(X, mobility_df, on='date', how='outer')\n",
        "X = pd.merge(X, tweet_df, on='date', how='outer')\n",
        "\n",
        "# add lagged Y columns\n",
        "prediction_length = 7\n",
        "for i in range(prediction_length):\n",
        "    X[f'Y{i}'] = X[state].shift(-i)\n",
        "\n",
        "X.dropna(inplace=True)\n",
        "display(X[X[state] <= 0])\n",
        "dates = X['date']\n",
        "X.drop(columns=['date'], inplace=True)\n",
        "\n",
        "# select the target columns\n",
        "Y = X.iloc[:, -prediction_length:].values\n",
        "# drop the target\n",
        "X = X.iloc[:, :-prediction_length]\n",
        "n_features = X.shape[-1]\n",
        "X = X.values\n",
        "X_train, X_CV = X[:450], X[450:]\n",
        "Y_train, Y_CV = Y[:450], Y[450:]\n",
        "dates_train, dates_CV = dates[:450], dates[450:]\n",
        "\n",
        "window_size = 14\n",
        "generator_train = TimeseriesGenerator(X_train, Y_train, length=window_size)\n",
        "generator_CV = TimeseriesGenerator(X_CV, Y_CV, length=window_size)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Hello Hello!\")"
      ],
      "metadata": {
        "id": "MgoaxxoA4pNr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb79e20b-2425-4e50-a7dd-bb3b4b4afa08"
      },
      "id": "MgoaxxoA4pNr",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello Hello!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5b2709d",
      "metadata": {
        "id": "f5b2709d"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}